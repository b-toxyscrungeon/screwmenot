{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Machine Learning Assignment - 2\n### GROUP 256\n### PS - 6\n### Group Members -\n* P.V.Vihari (2022AC05593)\n* Manas Tuteja (2022AC05507)\n* Godavarthi Krishna Vamsi (2022AC05704)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n# Stats\nimport scipy.stats as ss\nfrom scipy import interp\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Time\nfrom contextlib import contextmanager\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-17T13:04:46.228090Z","iopub.execute_input":"2023-09-17T13:04:46.228409Z","iopub.status.idle":"2023-09-17T13:04:51.304782Z","shell.execute_reply.started":"2023-09-17T13:04:46.228385Z","shell.execute_reply":"2023-09-17T13:04:51.303755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading Dataset from CSV","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/pima-indian-diabetes-data/Assignment 2.6 - Data/diabetes.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.316396Z","iopub.execute_input":"2023-09-17T13:04:51.316794Z","iopub.status.idle":"2023-09-17T13:04:51.334586Z","shell.execute_reply.started":"2023-09-17T13:04:51.316770Z","shell.execute_reply":"2023-09-17T13:04:51.333519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.417499Z","iopub.execute_input":"2023-09-17T13:04:51.418098Z","iopub.status.idle":"2023-09-17T13:04:51.434571Z","shell.execute_reply.started":"2023-09-17T13:04:51.418064Z","shell.execute_reply":"2023-09-17T13:04:51.433070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.461989Z","iopub.execute_input":"2023-09-17T13:04:51.462223Z","iopub.status.idle":"2023-09-17T13:04:51.468325Z","shell.execute_reply.started":"2023-09-17T13:04:51.462202Z","shell.execute_reply":"2023-09-17T13:04:51.467426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.484842Z","iopub.execute_input":"2023-09-17T13:04:51.485136Z","iopub.status.idle":"2023-09-17T13:04:51.493453Z","shell.execute_reply.started":"2023-09-17T13:04:51.485107Z","shell.execute_reply":"2023-09-17T13:04:51.492689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.516251Z","iopub.execute_input":"2023-09-17T13:04:51.516971Z","iopub.status.idle":"2023-09-17T13:04:51.534290Z","shell.execute_reply.started":"2023-09-17T13:04:51.516934Z","shell.execute_reply":"2023-09-17T13:04:51.533231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"import pandas_profiling as df_report\n\ndf_report.ProfileReport(df)\n\n#Below is an HTML based dashboard/report on the dataset, please use inner scroll bar to navigate","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:04:51.567332Z","iopub.execute_input":"2023-09-17T13:04:51.567694Z","iopub.status.idle":"2023-09-17T13:05:18.695425Z","shell.execute_reply.started":"2023-09-17T13:04:51.567661Z","shell.execute_reply":"2023-09-17T13:05:18.694214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  As observed from the profiling report:\n* Pregnancies is highly overall correlated with Age\n* SkinThickness is highly overall correlated with Insulin\n* Insulin is highly overall correlated with SkinThickness\n* Age is highly overall correlated with Pregnancies","metadata":{}},{"cell_type":"markdown","source":"Correlated features in general don't improve models (although it depends on the specifics of the problem like the number of variables and the degree of correlation), but they affect specific models in different ways and to varying extents:\n\nFor linear models (e.g., linear regression or logistic regression), multicolinearity can yield solutions that are wildly varying and possibly numerically unstable.\n\nRandom forests can be good at detecting interactions between different features, but highly correlated features can mask these interactions.","metadata":{}},{"cell_type":"markdown","source":"### **Why a correlation analysis was needed?**","metadata":{}},{"cell_type":"markdown","source":"Correlational analysis is used to measure the strength and direction of the relationship between two variables. When building an LR model, it’s important to check for multicollinearity, which occurs when two or more independent variables are highly correlated with each other. Multicollinearity can cause problems in the model, such as unstable coefficient estimates and difficulty in interpreting the effects of individual predictors.\n\nBy performing a correlational analysis, we can identify pairs of highly correlated variables and take appropriate action to address multicollinearity. This may involve removing one of the correlated variables from the model or combining them into a single predictor.","metadata":{}},{"cell_type":"markdown","source":"## What problems does Multicollinearity cause?\nMulticollinearity causes the following two basic types of problems:\n\nThe coefficient estimates can swing wildly based on which other\nindependent variables are in the model. The coefficients become very sensitive to small changes in the model.\nMulticollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.","metadata":{}},{"cell_type":"markdown","source":"## The need to reduce multicollinearity \n\nIt depends on its severity and your primary goal of a predictive model. \n\n* The severity of the problems increases with the degree of the multicollinearity. Therefore, if there is only moderate multicollinearity, it is not always needed to be resolved.\n\n* Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that we are particularly interested in, we may not need to resolve it. Suppose your model contains the experimental variables of interest and some control variables. If high multicollinearity exists for the control variables but not the experimental variables, then you can interpret the experimental variables without problems.\n\n* Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If the primary goal is to make predictions, and don’t need to understand the role of each independent variable, we don’t need to reduce severe multicollinearity.","metadata":{}},{"cell_type":"markdown","source":"### The team will split try 1st iteration of Model Building with 2 cases","metadata":{}},{"cell_type":"markdown","source":"#### ITR 1. A \n* Create Random Forest and KNN models on dataset with correlated feature\n\n#### ITR 1. B\n* Create Random Forest and KNN models on dataset after removing all correlated feature","metadata":{}},{"cell_type":"markdown","source":"This is done to validate the hypothesis that Random Forest Classifiers are less effected with Multi collienarity and should yield similar results in both cases. If the model perfromance falls in the case B of ITR1, it would indicate that the model hasn't learnt propeerly due to a lack of information","metadata":{}},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Outlier Detection","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:18.733060Z","iopub.execute_input":"2023-09-17T13:05:18.733483Z","iopub.status.idle":"2023-09-17T13:05:18.761115Z","shell.execute_reply.started":"2023-09-17T13:05:18.733459Z","shell.execute_reply":"2023-09-17T13:05:18.760305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Columns - **Pregnancies, Skin Thickness, Insulin, and DiabetesPedigreeFunction** have high number of outliers as can be seen in the table above. We can observe a very high Max value when compared to 25%ile, Mean and 75%ile","metadata":{}},{"cell_type":"markdown","source":"### Scatter Plot for Better Visualizations","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df, x= 'Pregnancies', y = 'Pregnancies')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:18.777847Z","iopub.execute_input":"2023-09-17T13:05:18.778060Z","iopub.status.idle":"2023-09-17T13:05:19.012881Z","shell.execute_reply.started":"2023-09-17T13:05:18.778039Z","shell.execute_reply":"2023-09-17T13:05:19.012080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df, x= 'Pregnancies')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.020297Z","iopub.execute_input":"2023-09-17T13:05:19.020599Z","iopub.status.idle":"2023-09-17T13:05:19.278982Z","shell.execute_reply.started":"2023-09-17T13:05:19.020574Z","shell.execute_reply":"2023-09-17T13:05:19.277917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observe that the Pregnancies column has some outright outliers where some females have as high as 17 pregnancies. If the outlier is not treated it will increase the variance of the training data unrealistically and give a skewed understanding of Central Tendencies like Mean. Hence the team recommends Normalization method to scale down all the values between 0 and 1 and reduce the impact of high variance on the model's predictions","metadata":{}},{"cell_type":"markdown","source":"### Using Min-Max Normalization to scale down the Pregnancies Feature","metadata":{}},{"cell_type":"code","source":"df_min_max_scaled = df.copy()\n  \n# apply normalization techniques by Column 1\ncolumn = 'Pregnancies'\ndf_min_max_scaled[column] = (df_min_max_scaled[column] - df_min_max_scaled[column].min()) / (df_min_max_scaled[column].max() - df_min_max_scaled[column].min())    \n  \n# view normalized data\ndisplay(df_min_max_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.289854Z","iopub.execute_input":"2023-09-17T13:05:19.290278Z","iopub.status.idle":"2023-09-17T13:05:19.308052Z","shell.execute_reply.started":"2023-09-17T13:05:19.290249Z","shell.execute_reply":"2023-09-17T13:05:19.307148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Insulin Feature Outliers","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df, x= 'Insulin', y = 'Insulin')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.323049Z","iopub.execute_input":"2023-09-17T13:05:19.323363Z","iopub.status.idle":"2023-09-17T13:05:19.503741Z","shell.execute_reply.started":"2023-09-17T13:05:19.323340Z","shell.execute_reply":"2023-09-17T13:05:19.502448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is easily observable by the scatterplot that there are **some very extreme values in the Insulin column**. *Normalization may not be able to reduce* the variance here to make legitimate predictions. Hence, the team will **use Inter-Quartile Range to remove outliers from the column**. ","metadata":{}},{"cell_type":"markdown","source":"As per the scatter plot, 600 is a good MAX for the column and all the values above it are generally considered outliers. Hence we draw the boundary at 3*IQR ","metadata":{}},{"cell_type":"code","source":"Q1 = df_min_max_scaled['Insulin'].quantile(0.25)\nQ3 = df_min_max_scaled['Insulin'].quantile(0.75)\nIQR = Q3 - Q1\nlower = Q1 - 3*IQR\nupper = Q3 + 3*IQR\n \n# Create arrays of Boolean values indicating the outlier rows\nupper_array = np.where(df_min_max_scaled['Insulin']>=upper)[0]\nlower_array = np.where(df_min_max_scaled['Insulin']<=lower)[0]\n \n# Removing the outliers\ndf_min_max_scaled.drop(index=upper_array, inplace=True)\ndf_min_max_scaled.drop(index=lower_array, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.516616Z","iopub.execute_input":"2023-09-17T13:05:19.516894Z","iopub.status.idle":"2023-09-17T13:05:19.527476Z","shell.execute_reply.started":"2023-09-17T13:05:19.516871Z","shell.execute_reply":"2023-09-17T13:05:19.526101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(df_min_max_scaled, x= 'Insulin', y = 'Insulin')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.542204Z","iopub.execute_input":"2023-09-17T13:05:19.542595Z","iopub.status.idle":"2023-09-17T13:05:19.740081Z","shell.execute_reply.started":"2023-09-17T13:05:19.542573Z","shell.execute_reply":"2023-09-17T13:05:19.738224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers have been treated very well as observed from the scatterplot","metadata":{}},{"cell_type":"code","source":"df_min_max_scaled.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.753559Z","iopub.execute_input":"2023-09-17T13:05:19.754128Z","iopub.status.idle":"2023-09-17T13:05:19.782461Z","shell.execute_reply.started":"2023-09-17T13:05:19.754099Z","shell.execute_reply":"2023-09-17T13:05:19.781381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skin Thickness Feature","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df_min_max_scaled, x= 'SkinThickness', y = 'SkinThickness')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:19.802599Z","iopub.execute_input":"2023-09-17T13:05:19.802900Z","iopub.status.idle":"2023-09-17T13:05:19.993996Z","shell.execute_reply.started":"2023-09-17T13:05:19.802872Z","shell.execute_reply":"2023-09-17T13:05:19.992730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It is easily observable by the scatterplot that there are **some very extreme values in the SkinThickness column**. *Normalization may not be able to reduce* the variance here to make legitimate predictions. Hence, the team will **use Inter-Quartile Range to remove outliers from the column**. ","metadata":{}},{"cell_type":"markdown","source":"As per the scatterplot 0.5 would be a good estimate to eliminate outliers","metadata":{}},{"cell_type":"code","source":"Q1 = df_min_max_scaled['SkinThickness'].quantile(0.01)\nQ3 = df_min_max_scaled['SkinThickness'].quantile(0.99)\nIQR = Q3 - Q1\nlower = Q1 - 0.01*IQR\nupper = Q3 + 0.01*IQR\n \n# Create arrays of Boolean values indicating the outlier rows\nupper_array = np.where(df_min_max_scaled['SkinThickness']>=upper)[0]\nlower_array = np.where(df_min_max_scaled['SkinThickness']<=lower)[0]\n \n# Removing the outliers\ndf_min_max_scaled.drop(index=upper_array, inplace=True)\ndf_min_max_scaled.drop(index=lower_array, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.005836Z","iopub.execute_input":"2023-09-17T13:05:20.006186Z","iopub.status.idle":"2023-09-17T13:05:20.017306Z","shell.execute_reply.started":"2023-09-17T13:05:20.006138Z","shell.execute_reply":"2023-09-17T13:05:20.016480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(df_min_max_scaled, x= 'SkinThickness', y = 'SkinThickness')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.022370Z","iopub.execute_input":"2023-09-17T13:05:20.022667Z","iopub.status.idle":"2023-09-17T13:05:20.240249Z","shell.execute_reply.started":"2023-09-17T13:05:20.022639Z","shell.execute_reply":"2023-09-17T13:05:20.239519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DiabetesPedigreeFunction Outlier Removal","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df_min_max_scaled, x= 'DiabetesPedigreeFunction', y = 'DiabetesPedigreeFunction')","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.249831Z","iopub.execute_input":"2023-09-17T13:05:20.250136Z","iopub.status.idle":"2023-09-17T13:05:20.472649Z","shell.execute_reply.started":"2023-09-17T13:05:20.250106Z","shell.execute_reply":"2023-09-17T13:05:20.471582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We can observe that the DiabetesPedigreeFunction column has some outright outliers. If the outlier is not treated it will increase the variance of the training data unrealistically and give a skewed understanding of Central Tendencies like Mean. Hence the team recommends Normalization method to scale down all the values between 0 and 1 and reduce the impact of high variance on the model's predictions","metadata":{}},{"cell_type":"code","source":"df_final = df_min_max_scaled.copy()\n  \n# apply normalization techniques by Column 1\ncolumn = 'DiabetesPedigreeFunction'\ndf_final[column] = (df_final[column] - df_final[column].min()) / (df_final[column].max() - df_final[column].min())    \n  \n# view normalized data\ndisplay(df_final)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.480713Z","iopub.execute_input":"2023-09-17T13:05:20.481412Z","iopub.status.idle":"2023-09-17T13:05:20.499716Z","shell.execute_reply.started":"2023-09-17T13:05:20.481381Z","shell.execute_reply":"2023-09-17T13:05:20.498376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.513023Z","iopub.execute_input":"2023-09-17T13:05:20.513379Z","iopub.status.idle":"2023-09-17T13:05:20.544496Z","shell.execute_reply.started":"2023-09-17T13:05:20.513349Z","shell.execute_reply":"2023-09-17T13:05:20.543393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **The outliers have been treated!**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.554723Z","iopub.execute_input":"2023-09-17T13:05:20.555781Z","iopub.status.idle":"2023-09-17T13:05:20.560535Z","shell.execute_reply.started":"2023-09-17T13:05:20.555749Z","shell.execute_reply":"2023-09-17T13:05:20.559051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_modeling = df_final.copy()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.564986Z","iopub.execute_input":"2023-09-17T13:05:20.565641Z","iopub.status.idle":"2023-09-17T13:05:20.574555Z","shell.execute_reply.started":"2023-09-17T13:05:20.565609Z","shell.execute_reply":"2023-09-17T13:05:20.573807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking for Sample Balancing","metadata":{}},{"cell_type":"markdown","source":"Data balancing is a crucial step in preparing your dataset for machine learning models, especially when dealing with imbalanced datasets. Here are some commonly used techniques:\n\n* **Resampling**: This involves either oversampling the minority class or undersampling the majority class to balance the dataset.\n\n* **SMOTE** (Synthetic Minority Over-sampling Technique): This method generates synthetic examples of the minority class to balance the dataset.\n\n* **Class Weights**: Some machine learning models provide a parameter called class_weights, which can be used to give more importance to the minority class during training.\n\n* **Ensemble Methods**: Techniques like bagging and boosting can be adapted for imbalanced data.\n\n* **Cost-Sensitive Learning**: This involves incorporating misclassification costs or using cost-sensitive algorithms.","metadata":{}},{"cell_type":"code","source":"df_modeling['Outcome'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.592983Z","iopub.execute_input":"2023-09-17T13:05:20.593336Z","iopub.status.idle":"2023-09-17T13:05:20.600745Z","shell.execute_reply.started":"2023-09-17T13:05:20.593306Z","shell.execute_reply":"2023-09-17T13:05:20.599723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The team will tackle the problem of Data Imbalance by using the class-weight method as the underlying sample population in the Dataset is quite low and we do not wish to lose our original sample.","metadata":{}},{"cell_type":"markdown","source":"#### As we can see, 64% of the Sample is mapped to Outcome Variable Value = 0 and only 36% is mapped to Value =1. \n\n#### This can be tackled by moving the positive class prediction threshold to 0.36 to help balance out the predictions v/s the training sample and help us evaluate our Classification Models properly.","metadata":{}},{"cell_type":"markdown","source":"### 80-20 Train-Test Split","metadata":{}},{"cell_type":"code","source":"x1=df_modeling.iloc[:,:-1]\ny1=df_modeling.iloc[:,-1]","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.616871Z","iopub.execute_input":"2023-09-17T13:05:20.617099Z","iopub.status.idle":"2023-09-17T13:05:20.621864Z","shell.execute_reply.started":"2023-09-17T13:05:20.617077Z","shell.execute_reply":"2023-09-17T13:05:20.621055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input Features","metadata":{}},{"cell_type":"code","source":"x1","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.635130Z","iopub.execute_input":"2023-09-17T13:05:20.635408Z","iopub.status.idle":"2023-09-17T13:05:20.649864Z","shell.execute_reply.started":"2023-09-17T13:05:20.635387Z","shell.execute_reply":"2023-09-17T13:05:20.649217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Variable","metadata":{}},{"cell_type":"code","source":"y1","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:20.888661Z","iopub.execute_input":"2023-09-17T13:05:20.888994Z","iopub.status.idle":"2023-09-17T13:05:20.895859Z","shell.execute_reply.started":"2023-09-17T13:05:20.888968Z","shell.execute_reply":"2023-09-17T13:05:20.895117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size=0.20, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:22.598779Z","iopub.execute_input":"2023-09-17T13:05:22.599137Z","iopub.status.idle":"2023-09-17T13:05:22.606716Z","shell.execute_reply.started":"2023-09-17T13:05:22.599111Z","shell.execute_reply":"2023-09-17T13:05:22.605225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using dataset with the correlated features ITR 1 A\n### Initializing Random Forest Model ITR 1 A","metadata":{}},{"cell_type":"code","source":"rf_1 = RandomForestClassifier().fit(X_train, y_train)\nrf_y_pred = rf_1.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:24.983617Z","iopub.execute_input":"2023-09-17T13:05:24.983956Z","iopub.status.idle":"2023-09-17T13:05:25.187873Z","shell.execute_reply.started":"2023-09-17T13:05:24.983929Z","shell.execute_reply":"2023-09-17T13:05:25.186297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:26.680396Z","iopub.execute_input":"2023-09-17T13:05:26.680748Z","iopub.status.idle":"2023-09-17T13:05:26.685824Z","shell.execute_reply.started":"2023-09-17T13:05:26.680723Z","shell.execute_reply":"2023-09-17T13:05:26.684599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict_class = [1 if prob > 0.36 else 0 for prob in rf_y_pred]\n# Accuracy Score\nlm_score  =  accuracy_score(y_test, y_predict_class)\nprint('Accuracy Score is: ', lm_score)\n\n# Confusion Matrix\nprint(\"Confusion Matrix\\n\",confusion_matrix(y_test, y_predict_class))\n\n# Classificaion Report\nprint(\"Classification Report of Random Forest :\\n\",classification_report(y_test, y_predict_class))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:36.230466Z","iopub.execute_input":"2023-09-17T13:05:36.230964Z","iopub.status.idle":"2023-09-17T13:05:36.244986Z","shell.execute_reply.started":"2023-09-17T13:05:36.230938Z","shell.execute_reply":"2023-09-17T13:05:36.243843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using dataset without the correlated features - ITR 1B","metadata":{}},{"cell_type":"markdown","source":"* Removing 1 feature between Pregnancies, Age randomly\n* Removing 1 feature between SkinThickness, Insulin randomly","metadata":{}},{"cell_type":"code","source":"df_modeling.columns","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:05:43.791022Z","iopub.execute_input":"2023-09-17T13:05:43.791916Z","iopub.status.idle":"2023-09-17T13:05:43.800354Z","shell.execute_reply.started":"2023-09-17T13:05:43.791869Z","shell.execute_reply":"2023-09-17T13:05:43.799129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x2 = df_modeling[['Glucose', 'SkinThickness', 'BloodPressure','BMI', 'DiabetesPedigreeFunction','Pregnancies', 'Outcome']]\ny2 = y1","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:08:12.942201Z","iopub.execute_input":"2023-09-17T13:08:12.942537Z","iopub.status.idle":"2023-09-17T13:08:12.949873Z","shell.execute_reply.started":"2023-09-17T13:08:12.942511Z","shell.execute_reply":"2023-09-17T13:08:12.948646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing Random Forest Model ITR 1 B","metadata":{}},{"cell_type":"code","source":"X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(x2, y2, test_size=0.20, random_state=0)\n\nrf_2 = RandomForestClassifier().fit(X_train_2, y_train_2)\nrf_y_pred_2 = rf_2.predict(X_test_2)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:08:14.325300Z","iopub.execute_input":"2023-09-17T13:08:14.325635Z","iopub.status.idle":"2023-09-17T13:08:14.484887Z","shell.execute_reply.started":"2023-09-17T13:08:14.325609Z","shell.execute_reply":"2023-09-17T13:08:14.483309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict_class_2 = [1 if prob > 0.64 else 0 for prob in rf_y_pred_2]\n# Accuracy Score\nlm_score  =  accuracy_score(y_test_2, y_predict_class_2)\nprint('Accuracy Score is: ', lm_score)\n\n# Confusion Matrix\nprint(\"Confusion Matrix\\n\",confusion_matrix(y_test_2, y_predict_class_2))\n\n# Classificaion Report\nprint(\"Classification Report of Random Forest :\\n\",classification_report(y_test_2, y_predict_class_2))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:08:33.982478Z","iopub.execute_input":"2023-09-17T13:08:33.983086Z","iopub.status.idle":"2023-09-17T13:08:34.002781Z","shell.execute_reply.started":"2023-09-17T13:08:33.983055Z","shell.execute_reply":"2023-09-17T13:08:34.001033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing the Accuracies, Precision and Recall between ITR 1A and ITR 1B of Random Forest Classifier","metadata":{}},{"cell_type":"markdown","source":"#### It is observed that not using Correlated Features, the Random Forest Classifier starts overfitting on the small size of sample and won't do very well with new values\n\nThis is because\n* Tree Based algorithms are generally good at dealing with Multicollinearity as they don't use all the features at the same time to make the split \n\n* Hence multicollinearity does not have a massive effect on the predictions of a model is validated\n\n* By removing important features, a good amount of information was hidden from the model and hence it started overfitting the sample","metadata":{}},{"cell_type":"markdown","source":"## Trying out Random Forest Iterations with Different Train Test Split with all the features ","metadata":{}},{"cell_type":"markdown","source":"* **90-10 Train Test Split**","metadata":{}},{"cell_type":"code","source":"X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(x1, y1, test_size=0.10, random_state=0)\nrf_3 = RandomForestClassifier().fit(X_train_3, y_train_3)\nrf_y_pred_3 = rf_3.predict(X_test_3)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:10:36.319485Z","iopub.execute_input":"2023-09-17T13:10:36.319794Z","iopub.status.idle":"2023-09-17T13:10:36.511252Z","shell.execute_reply.started":"2023-09-17T13:10:36.319769Z","shell.execute_reply":"2023-09-17T13:10:36.509877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict_class_3 = [1 if prob > 0.36 else 0 for prob in rf_y_pred_3]\n# Accuracy Score\nlm_score  =  accuracy_score(y_test_3, y_predict_class_3)\nprint('Accuracy Score is: ', lm_score)\n\n# Confusion Matrix\nprint(\"Confusion Matrix\\n\",confusion_matrix(y_test_3, y_predict_class_3))\n\n# Classificaion Report\nprint(\"Classification Report of Random Forest :\\n\",classification_report(y_test_3, y_predict_class_3))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:10:47.631498Z","iopub.execute_input":"2023-09-17T13:10:47.631848Z","iopub.status.idle":"2023-09-17T13:10:47.650706Z","shell.execute_reply.started":"2023-09-17T13:10:47.631819Z","shell.execute_reply":"2023-09-17T13:10:47.649734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **The Random Forest Classifier performs poorly with a larger training sample as it starts overfitting**","metadata":{}},{"cell_type":"markdown","source":"## Trying out Random Forest Iterations with Hyperparameter Tuning using Cross Validations","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier's implementation in Scikit learn, as used here in the notebook has the following hyperparameters that can be tuned:\n\n* **n_estimators**: This parameter controls the number of trees in the forest. More trees can help to get a more generalized result, but it can also increase the time complexity of the model12.\n\n* **max_depth**: This parameter governs the maximum height up to which the trees inside the forest can grow. It is crucial for increasing the accuracy of the model, but setting it too high can lead to overfitting12.\n\n* **min_samples_split**: This specifies the minimum number of samples an internal node must hold in order to split into further nodes. A very low value can lead to overfitting, while a very high value can cause underfitting12.\n\n* **min_samples_leaf**: This is the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches1.\n\n* **max_features**: This is the number of features to consider when looking for the best split1. **This hyperparameter has been deprecated in the latest versions of SciKit Learn hence won't be tuned.**","metadata":{}},{"cell_type":"markdown","source":"* **Creating a Random Grid of Hyperparameters' Values**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               \n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:36:22.749154Z","iopub.execute_input":"2023-09-17T13:36:22.749506Z","iopub.status.idle":"2023-09-17T13:36:22.757914Z","shell.execute_reply.started":"2023-09-17T13:36:22.749476Z","shell.execute_reply":"2023-09-17T13:36:22.756875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Radom Search Training**","metadata":{}},{"cell_type":"markdown","source":"The most important arguments in RandomizedSearchCV are **n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively).** More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.","metadata":{}},{"cell_type":"code","source":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf_h = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf_h, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:36:26.866699Z","iopub.execute_input":"2023-09-17T13:36:26.867253Z","iopub.status.idle":"2023-09-17T13:39:18.891245Z","shell.execute_reply.started":"2023-09-17T13:36:26.867223Z","shell.execute_reply":"2023-09-17T13:39:18.889974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **View Best Combination of Hyperparameters**","metadata":{}},{"cell_type":"code","source":"rf_random.best_params_","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:43:04.617132Z","iopub.execute_input":"2023-09-17T13:43:04.617534Z","iopub.status.idle":"2023-09-17T13:43:04.623837Z","shell.execute_reply.started":"2023-09-17T13:43:04.617496Z","shell.execute_reply":"2023-09-17T13:43:04.622867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing Base Model v/s Best Random Search Model","metadata":{}},{"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors / test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\n\n# Using Base Model rf_1 here\nbase_accuracy = evaluate(rf_1, X_test, y_test)\n\n#Using best random search hyperparameters arrived at using Cross Validation\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:51:24.939094Z","iopub.execute_input":"2023-09-17T13:51:24.939501Z","iopub.status.idle":"2023-09-17T13:51:24.974013Z","shell.execute_reply.started":"2023-09-17T13:51:24.939474Z","shell.execute_reply":"2023-09-17T13:51:24.972896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **The model with Hyperparameters are given by Cross Validations are performing better than that of the base model**\n\n* For hyperparameter tuning, we perform many iterations of the entire K-Fold CV process, each time using different model settings. We then compare all of the models, select the best one, train it on the full training set, and then evaluate on the testing set.\n* To assess a different set of hyperparameters, we have to split our training data into K fold and train and evaluate K times. Since we have 100 sets of hyperparameters and are using 3-Fold CV, that represents 300 training loops.","metadata":{}},{"cell_type":"markdown","source":"### Initializing KNN Model ITR 1 A\n#### With multicollinearity","metadata":{}},{"cell_type":"markdown","source":"* With Hyperparameter Tuning (K) ","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,9)\nknn_train_accuracy =np.empty(len(neighbors))\nknn_test_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    knn_train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    knn_test_accuracy[i] = knn.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:13:26.735055Z","iopub.execute_input":"2023-09-17T13:13:26.735424Z","iopub.status.idle":"2023-09-17T13:13:27.066092Z","shell.execute_reply.started":"2023-09-17T13:13:26.735395Z","shell.execute_reply":"2023-09-17T13:13:27.065175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(knn_train_accuracy, knn_test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:14:09.395956Z","iopub.execute_input":"2023-09-17T13:14:09.397093Z","iopub.status.idle":"2023-09-17T13:14:09.404498Z","shell.execute_reply.started":"2023-09-17T13:14:09.397064Z","shell.execute_reply":"2023-09-17T13:14:09.403511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Observation\n* **Test Accuracy of the KNN Algorithm Increases with a higher value of K**","metadata":{}},{"cell_type":"markdown","source":"### Initializing KNN Model ITR 1 B\n\n#### Without Multicollinearity","metadata":{}},{"cell_type":"markdown","source":"* With Hyperparameter Tuning (K) ","metadata":{}},{"cell_type":"code","source":"neighbors = np.arange(1,9)\nknn_train_accuracy_2 =np.empty(len(neighbors))\nknn_test_accuracy_2 = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    #Fit the model\n    knn.fit(X_train_2, y_train_2)\n    \n    #Compute accuracy on the training set\n    knn_train_accuracy_2[i] = knn.score(X_train_2, y_train_2)\n    \n    #Compute accuracy on the test set\n    knn_test_accuracy_2[i] = knn.score(X_test_2, y_test_2)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:16:16.095951Z","iopub.execute_input":"2023-09-17T13:16:16.096327Z","iopub.status.idle":"2023-09-17T13:16:16.430867Z","shell.execute_reply.started":"2023-09-17T13:16:16.096299Z","shell.execute_reply":"2023-09-17T13:16:16.429569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(knn_train_accuracy_2, knn_test_accuracy_2)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T13:16:39.488269Z","iopub.execute_input":"2023-09-17T13:16:39.488664Z","iopub.status.idle":"2023-09-17T13:16:39.498275Z","shell.execute_reply.started":"2023-09-17T13:16:39.488627Z","shell.execute_reply":"2023-09-17T13:16:39.497117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation\n\n* **We observe that the Test Accuracy of KNN Classifier decreases with an increasing K whent he features are treated for Multicollinearity**\n\n* This is because by removing important features, a good amount of information was hidden from the model and hence it started overfitting the sample","metadata":{}},{"cell_type":"markdown","source":"## Final Answer/Conclusion","metadata":{}},{"cell_type":"markdown","source":"### Observation","metadata":{}},{"cell_type":"markdown","source":"It is observed that the Random Forest Classifier performs much better than the KNN Classifier in terms of\n\n* Accuracy\n * RF - 68%\n * KNN - 78%\n \n \n* Recall\n * RF - 67%\n * KNN - 64%\n","metadata":{}},{"cell_type":"markdown","source":"### Justification\n\n* The **Random Forest model has a 0.67 Recall**. This means 67% of patients, who DID infact have diabetes, were correctly identified by the Model. This means if a total of 1000 people would have a diabetes, our Model had given the Label - 1 (hig risk) to 670 of those people this is far better when compared to the **KNN Model that has a recall of 0.64** which means if a total of 1000 people would have a diabetes, our Model had given the Label - 1 (hig risk) to 640.  \n\n* The **Random Forest Accuracy of 0.68** tells us that in general the Random Forest Model will make the right prediction 68% of the times when compared to **KNN Accuracy of 78%** which means KNN would make correct predictions 78% of the time. \n\n* However, Accuracy is not a measure of how much better is a model is identifying people with diabetes which is a more important metric since someone who doesn't have diabetes in the first place can infact be fine without being diagnoes.\n\n* In the world of medical diagnosis and healthcare, Recall is a much more important statistic to measure a model's performance as it is identifying most of the at-risk population correctly and helps in saving lives.\n\n\n## We would recommend going forward with the Random Forest Classifier trained by tuning hyperparmeters using cross validation as it best fits the use case.","metadata":{}}]}